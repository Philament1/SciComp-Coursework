%Template file for Scientific Computation project 2 discussion and figures
\documentclass{article}
\usepackage[a4paper, margin=1in]{geometry}
\title{Scientific Computation Project 2}
\usepackage{minted}
\usepackage{amsmath}
\usepackage{amssymb}
\newcommand{\trm}{\textrm}
\newcommand{\pa}{\partial}
\author{\emph{02027072}}

\usepackage{graphicx}

\begin{document}

\maketitle

%---------------- Part 1  -------------------
\hrule
\hrule

\subsection*{Part 1}

\subsubsection*{1.}
%Place your discussion for question 1 here
The problem the functions attempt to solve is to find a path in the input \textit{networkx} graph from a specified source node to a specified target node that minimises the weight of the maximum edge weight between two nodes along the path. Let $P(s,x)$ be the set of paths from node $s$ to node $x$, where a path $p$ is the set of edges $e$ along the path, and an edge has weight $w(e)$. Then the problem can be written as:
\begin{equation}
    w^* = \min_{p \in P(s,x)}\left( \max_{e \in p} w(e) \right) \label{eq:minimax}
\end{equation}
where $w^*$ is the value of this minimum maximum edge weight.

Both functions return the value of $w^*$, and \textit{searchGPT} also returns the optimal path as a list of nodes from source to target that the path traverses through. The strategy both functions use is an adjusted version of Dijkstra's algorithm with a priority queue, but instead of tracking the sum of the edge weights from each node to the source, it instead tracks the maximum edge weight on the path from each node to the source, which is then minimised. The priority queue is implemented with \textit{heapq} using \textit{heapq.push} to push elements to the list and \textit{heapq.pop} to pop elements from the list, such that they are ordered by maximum edge weight along the path to the node.


\subsubsection*{2.}
%Place your discussion for question 2 here
For a graph with positive weights where the source and target node are connected, both \textit{searchGPT} and \textit{searchPKR2} return the same correct value that solves the problem \eqref{eq:minimax}.

In the case where the source and target are disconnected or unreachable in the input graph, \textit{searchGPT} returns the value \textit{float(`inf')}, which can be interpreted as the minimum weight being infinite as there is no path. On the other hand, \textit{searchPKR2} returns a finite value with no connection to the problem \eqref{eq:minimax}, hence the method is inapplicable to disconnected source and target nodes. 

Both functions implement an altered form of Dijkstra's algorithm. From lectures, given a graph with $N$ nodes and $L$ edges, using Python $heapq$, we know that Dijkstra's has a computational cost of $\mathcal{O}((N+L)\log_2(N))$ for the search part of the algorithm.  On each iteration, whilst exploring the neighbours of the `current' node, when checking the edge weights of a new neighbour or updating a neighbour that has not yet been explored, the action of adding the new edge weight to the summed edge weights of the path to the current node is replaced with taking the maximum of the edge weight and the max edge weight of the current node. The cost of taking the $\max$ of two floats is the same as adding two floats, hence the cost of the search for both algorithms is still $\mathcal{O}((N+L)\log_2(N))$.

To return the path from source to target, \textit{searchGPT} creates a \textit{parents} dictionary, which stores and updates the parent node of each node that is reached. Then, if the target node is reached, a \textit{path} list is created by starting with the target node and iterating the insertion of the parent node to the start of the list until the source node is inserted. Suppose the path is of length $k$, then the creation of this list has a computational cost of $\mathcal{O}(k^2)$, as the \textit{insert} function has cost $\mathcal{O}(k)$. In the worst-case scenario, this path includes all nodes of the graph, so the time complexity would be $\mathcal{O}(N^2)$.

For my code in \textit{searchPKR2}, I introduce the \textit{parents} dictionary in a similar way to \textit{searchGPT}. Then if the target is found, a \textit{path} list is created from the \textit{parent} dictionary. Unlike \textit{searchGPT}, \textit{searchPKR2} begins with the target node and appends each parent to the end of the list until the source node is reached, then reverses the list. Suppose the path is of length $k$, then the looped \textit{append} function has a cost of $\mathcal{O}(k)$, followed by a \textit{reverse} function with a cost of $\mathcal{O}(k)$, so overall the computational cost of the path is of $\mathcal{O}(k)$. In the worst-case scenario, this path includes all nodes of the graph, so the time complexity of the \textit{path} list creation would be $\mathcal{O}(N)$. This is significantly better than \textit{searchGPT}.


%---------------- End Part 1 -------------------

\vspace{0.25in}

%---------------- Part 2  -------------------
\subsection*{Part 2}

\subsubsection*{1.}
%Place your discussion for question 1 here}
In my new implementation of \textit{part2q1}, I have adapted the form of the RHS of the model into \textit{NumPy} array form. I use a \textit{scipy} sparse matrix for the linear part of the RHS, then subtract the cubic term using element-wise broadcasting. This improves efficiency by removing the iteration over the list and utilising the fast wall time of \textit{scipy} sparse matrix multiplication and \text{NumPy} vector methods.

To efficiently compute the solution to the IVP, I use the \textit{scipy.integrate.solve\_ivp} function with the \textit{`BDF'} method. This method is highly robust and sophisticated as seen in lectures, and has a much faster wall-time. The default error tolerance is \textit{atol} is $10^{-6}$ as required, and we maximise its efficiency for large $n$ by setting the \textit{vectorized} parameter to \textit{True}.

\subsubsection*{2.}
%Place your discussion for question 2 here}

\begin{figure}[h!]
\centering
%Uncomment line below to display figure saved as fig1.png
\includegraphics[width=0.8\textwidth]{fig1.png}

\caption{Solution to initial condition $\mathbf{y_{0A}}$}
\label{fig1}
\end{figure}

From Figure \ref{fig1}, we observe that from initial condition $\mathbf{y_{0A}}$, the values converge towards $1$ at around $t=30$ and diverge away from its nearby equilibrium. 

\begin{figure}[h!]
\centering
%Uncomment line below to display figure saved as fig1.png
\includegraphics[width=0.8\textwidth]{fig2.png}

\caption{Solution to initial condition $\mathbf{y_{0B}}$}
\label{fig2}
\end{figure}

From Figure \ref{fig2}, we observe that from initial condition $\mathbf{y_{0B}}$, the values remain stable and close to the initial condition throughout. 

We assume that each initial condition $\mathbf{y_{0A}}$ and $\mathbf{y_{0B}}$ is ``close'' to an equilibrium point, so by applying \textit{scipy.optimize.root} onto the ODEs with initial guesses being the initial conditions, we find the equilibrium states $\overline{\mathbf{y_A}}$ and $\overline{\mathbf{y_B}}$. 

By introducing the power series expansion for $y_i$, where $\bar{y}_i$ is an equilibrium state and $0 < \epsilon \ll 1$:
\[
    y_i = \bar{y}_i + \epsilon\Tilde{y}_i + \mathcal{O}(\epsilon^2), i = 0, 1, \cdots, n-1
\]
we get the following linearised equations for the perturbations:
\begin{gather*}
    \frac{d\tilde{y}_0}{dt} = (\alpha - 3\bar{y}_0^2)\tilde{y_0} + \beta(\tilde{y}_{n-1} + \tilde{y}_1) \\
    \frac{d\tilde{y}_i}{dt} = (\alpha - 3\bar{y}_i^2)\tilde{y_i} + \beta(\tilde{y}_{i-1} + \tilde{y}_{i+1}), i=1, \cdots, n-1 \\
    \frac{d\tilde{y}_{n-1}}{dt} = (\alpha - 3\bar{y}_{n-1}^2)\tilde{y}_{n-1} + \beta(\tilde{y}_0 + \tilde{y}_{n-2})
\end{gather*}
which can be written in matrix form as
\[
    \frac{d\mathbf{\tilde{y}}}{dt} = \mathbf{M_{\bar{y}}\mathbf{\tilde{y}}}
\]
where

\[
    \mathbf{M_{\bar{y}}}
     =
    \begin{pmatrix}
        \alpha & \beta & & & \beta \\
        \beta & \alpha & \beta & &\\
        & \ddots & \ddots & \ddots & \\
        & & \beta & \alpha & \beta \\        
        \beta & & & \beta & \alpha 
    \end{pmatrix}
    -
    \begin{pmatrix}
        3\bar{y}_0^2 & & & & \\
        & 3\bar{y}_1^2 & & &\\
        & & \ddots & & \\
        & & & 3\bar{y}_{n-2}^2 & \\        
        & & & & 3\bar{y}_{n-1}^2
    \end{pmatrix}
\]

We can then compute the $n$ eigenvalues $\mathbf{\lambda}_i$ and eigenvectors $\mathbf{v}_i$ of $\mathbf{M_{\bar{y}}}$ to find a general solution of the form
\[
    \mathbf{\Tilde{y}} = c_1\mathbf{v}_1\exp{\lambda_1t} + \cdots + c_n\mathbf{v}_n\exp{\lambda_nt}
\]
We find $c_i, i=1, \cdots, n$ that satisfy the initial conditions $\mathbf{y}_0$ by solving the linear equations:
\[
    \mathbf{V}\mathbf{c} = \mathbf{y}_0
\]

where $\mathbf{V}$ is a $n \times n$ matrix with columns as the eigenvectors, and $\mathbf{c}$ is a column vector of $c_i$.

\begin{figure}[h!]
\centering
%Uncomment line below to display figure saved as fig1.png
\includegraphics[width=0.8\textwidth]{fig3.png}

\caption{Perturbations of equilibrium $\overline{\mathbf{y_A}}$}
\label{fig3}
\end{figure}

From Figure \ref{fig3}, we observe that perturbations from the equilibrium point $\overline{\mathbf{y_A}}$ appear to initially converge before diverging at around $t=$, which shows it is unstable. The

\begin{figure}[h!]
\centering
%Uncomment line below to display figure saved as fig1.png
\includegraphics[width=0.8\textwidth]{fig4.png}

\caption{Perturbations of equilibrium $\overline{\mathbf{y_B}}$}
\label{fig4}
\end{figure}

From Figure \ref{fig4}, perturbations from the equilibrium point $\overline{\mathbf{y_B}}$ appear to converge very quickly, which indicates it is stable for at least $t <= 40$.

\subsubsection*{3.}
%Place your discussion for question 3 here}

%Add additional figures if needed
%---------------- End Part 2 -------------------


\hrule
\hrule



%---------------- End document -------------------


\end{document}
