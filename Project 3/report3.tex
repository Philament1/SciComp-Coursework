%Template file for Scientific Computation project 3 discussion and figures
\documentclass{article}
\usepackage[a4paper, margin=1in]{geometry}
\title{Scientific Computation Project 3}
\usepackage{minted}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{placeins}
\usepackage{subcaption}
\newcommand{\trm}{\textrm}
\newcommand{\pa}{\partial}
\author{\emph{02027072}}

\usepackage{graphicx}

\begin{document}

\maketitle

%---------------- Part 1  -------------------
\hrule
\hrule

\subsection*{Part 1}
%place discussion for Part 1 here

We begin by reshaping $u$ into a $2304 \times 365$ \textit{NumPy} array $\mathbf{A}$, with each day as a datapoint in the columns, and locations (longitude and latitude combinations) as the attributes in the rows, and we apply PCA to the flattened 2-dimensional data. This allows us to spot temporal trends in wind speed.

When applying PCA, we note that the number of non-zero singular values is 364, which shows that the matrix $\mathbf{A}$ is not full rank as we have 365 days. 

\begin{figure}[h!]
\centering
\includegraphics[width=\textwidth]{fig_pc1.png}
\caption{First 3 PCs}
\label{fig_pc1}
\end{figure}

Figure \ref{fig_pc1} shows the first 3 principal components, i.e. the locations in which the highest variance in wind speed over time is captured.

\begin{figure}[h!]
    \begin{minipage}{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{fig_var1.png}
    \caption{Cumulative retained variance of PCs}
    \label{fig_var1}
    \end{minipage}%
    \hfill
    \begin{minipage}{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{fig_proj1.png}
    \caption{Projections onto first 2 PCs}
    \label{fig_proj1}
    \end{minipage}%
\end{figure}

From Figure \ref{fig_var1}, we observe that 80\% of the total variance is retained at around 20 principal components and that under 20\% of the total variance is captured by the first PC. 

Figure \ref{fig_proj1} shows our transformed data values as projections onto the first 2 principal components. There are no clear patterns in this so we do further analysis on this time series. 

\begin{figure}[h!]
    \begin{minipage}{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{fig_proj1b.png}
    \caption{FFT and Welch's on projection onto PC1}
    \label{fig_proj1b}
    \end{minipage}
    \hfill
    \begin{minipage}{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{fig_proj1c.png}
    \caption{Subsequent days of PC1 and PC2}
    \label{fig_proj1c}
    \end{minipage}%
\end{figure}

From Figure \ref{fig_proj1b}, we observe that none of the frequencies significantly dominate the others in both spectral density estimates, suggesting there are no consistent periodic patterns in wind speed over the year. The frequency density also appears to decrease as frequency increases, indicating that trends appear to be gradual and seasonal, and there are fewer high-frequency variations (rapid changes in wind speed). 

We see from Figure \ref{fig_proj1c} that there is a general positive correlation in wind speed between subsequent days. This also supports our earlier observation that there are few high-frequency variations. 

\FloatBarrier

We then transpose our array $\mathbf{A}$ into a $365 \times 2304$ \textit{NumPy} array, now treating the locations as the datapoints, and each day as an attribute. 

Figure \ref{fig_pc2} shows the first 2 principal components, i.e. the points in time that capture the most variance in space.

\begin{figure}[h!]
    \begin{minipage}{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{fig_pc2.png}
    \caption{First 2 Principal Components}
    \label{fig_pc2}
    \end{minipage}
    \hfill
    \begin{minipage}{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{fig_var2.png}
    \caption{Cumulative retained variance of PCs}
    \label{fig_var2}
    \end{minipage}
\end{figure}

From Figure \ref{fig_var2}, we observe that 80\% of the total variance is retained at around 10 principal components, and that the first principal component captures over 50\% of the total variance. 

\begin{figure}[h!]
\centering
\includegraphics[width=\textwidth]{fig_proj2.png}
\caption{Projected values of first 2 principal components}
\label{fig_proj2}
\end{figure}

From Figure \ref{fig_proj2}, we see a large variation in scores between the higher and lower latitudes, indicating a clear split in wind speed patterns between the upper and lower halves of the map. The second principal component also shows distinct zones where the scores are similar, indicating the presence of spatial clusters and patterns. There appears to be two distinct zones within around 175 to 250 longitude in both upper and lower halves that share similar wind speed patterns.  

%---------------- End Part 1 -------------------

\FloatBarrier
\vspace{0.25in}

%---------------- Part 2  -------------------
\subsection*{Part 2}

\subsubsection*{2.}
%Place your discussion for question 2 here}
For method 2, we solve the system of linear equations
\[
    \mathbf{A\tilde{f}} = \mathbf{Bf}
\]
for $\mathbf{\tilde{f}}$ with columns $\mathbf{\tilde{f}}_j = (\tilde{f}_{1/2,j}, \dots, \tilde{f}_{m-3/2,j})^\top$, where
\[
    \mathbf{A} = 
    \begin{pmatrix}
        1 \\
        \alpha & 1 & \alpha \\
        & \ddots & \ddots & \ddots \\
        && \alpha & 1 & \alpha \\
        &&&& 1
    \end{pmatrix}
\]
is a banded square matrix,
\[
    \mathbf{B} = 
    \begin{pmatrix}
        \tilde{a} & \tilde{b} & \tilde{c} & \tilde{d} \\
        \frac{b}{2} & \frac{a}{2} & \frac{a}{2} & \frac{b}{2} \\
        & \ddots & \ddots & \ddots & \ddots\\
        && \frac{b}{2} & \frac{a}{2} & \frac{a}{2} & \frac{b}{2} \\
        && \tilde{d} & \tilde{c} & \tilde{b} & \tilde{a}
    \end{pmatrix}
\]
is a rectangular banded matrix, and the columns of $\mathbf{f}$ are $\mathbf{f}_j = (f_{0,j}, \dots, f_{m-1,j})^\top$. 

To solve this linear equation efficiently, we convert matrix $\mathbf{A}$ to ``matrix diagonal ordered form'' and then apply \textit{scipy.linalg.solve\_banded}.

We apply both methods to the following functions on a 2-dimensional grid:
\begin{itemize}
    \item \textit{smooth oscillating function}
    \[
        f(x, y) = \sin(2\pi x)\cos(2\pi y)
    \]
    \item \textit{Franke's function - used as a test function for interpolation problems}
    \begin{align*}
        f(x, y) = & \frac{3}{4} \exp\left(-\frac{(9x - 2)^2}{4} - \frac{(9y - 2)^2}{4}\right) + \frac{3}{4} \exp\left(-\frac{(9x + 1)^2}{49} - \frac{(9y + 1)}{10}\right) \\
        & + \frac{1}{2} \exp\left(-\frac{(9x - 7)^2}{4} - \frac{(9y - 3)^2}{4}\right) - \frac{1}{5} \exp\left(-(9x - 4)^2 - (9y - 7)^2\right)
    \end{align*}
    
\end{itemize}

To determine the accuracy, cost and efficiency of both methods, we first apply them to each function and plot the errors as the absolute difference between the interpolated value and the actual value. We also measure the average wall times (over 1000 runs) and calculate the mean error (mean absolute difference between interpolated and calculated values). We test the function on a grid with sizes $n=50$ and $m=40$.

\FloatBarrier

\begin{figure}[h!]
\centering
\includegraphics[width=0.8\textwidth]{fig_osc.png}
\caption{Results for oscillating function}
\label{fig_osc}
\end{figure}

\begin{figure}[h!]
\centering
\includegraphics[width=0.8\textwidth]{fig_franke.png}
\caption{Results for Franke's function}
\label{fig_franke}
\end{figure}

From figures \ref{fig_osc} and \ref{fig_franke}, we observe that method 1 produces large errors at the peaks and troughs of both functions. Method 2 has a much more stable error value throughout both functions, except at the boundaries, where there is a significantly larger error value.

\begin{table}[h!]
  \centering
  \begin{tabular}{|c|c|c|}
    \hline
     & Error Method 1 & Error Method 2 \\
    \hline
    Oscillating function & 1.288e-03 & 1.234e-06 \\
    Franke's function & 5.093e-04 & 7.182e-07 \\
    \hline
  \end{tabular}
  \caption{Mean Error}
  \label{tab:part2_err}
\end{table}

From Table \ref{tab:part2_err}, we note that method 2 has a significantly lower error for both functions, hence it is more accurate overall despite the higher error on the boundaries.

\begin{table}[h!]
  \centering
  \begin{tabular}{|c|c|c|}
    \hline
     & Mean Wall Time (s) Method 1 & Mean Wall Time (s) Method 2 \\
    \hline
    Oscillating function & 7.026e-06 & 2.693e-04 \\
    Franke's function & 6.220e-06 & 3.413e-04 \\
    \hline
  \end{tabular}
  \caption{Wall Times}
  \label{tab:part2_walltimes}
\end{table}

From Table \ref{tab:part2_walltimes}, we see that method 2 has significantly longer wall times, as the method has a higher computational cost due to the use of matrix multiplication and solving linear equations, in comparison to the simpler method 1.

\FloatBarrier

We then test both methods on Franke's function for a range of different grid sizes (fixing $n=m+10$ for consistency).

\begin{figure}[h!]
\centering
\includegraphics[width=0.7\textwidth]{fig_yerror.png}
\caption{Mean Error for each $y$}
\label{fig_yerror}
\end{figure}

In Figure \ref{fig_yerror}, we test $m=40, 80, \dots, 160, 200$. We observe that the error is more consistent for different $y$ with method 1, whereas method 2 is significantly worse at the boundaries. Both methods are more accurate at larger $m$ values, and method 2 has error values that are magnitudes less than method 1.

\begin{figure}[h!]
\centering
\includegraphics[width=0.7\textwidth]{fig_merror.png}
\caption{Mean Error against $m$}
\label{fig_merror}
\end{figure}

In Figure \ref{fig_merror}, we test with larger $m$ values. The error for increasing $y$ is almost exponential for both methods, with the error for method 2 decreasing at a higher rate than method 1. We expect both error plots to gradually converge to a fixed error value for even greater $m$.

%Add additional figure if needed
%---------------- End Part 2 -------------------
\FloatBarrier
%---------------- Part 3  -------------------
\subsection*{Part 3}

\subsubsection*{1.}
%Place your discussion for question 1 here}
For this question, we perform analysis on $c=0.5, 1.2, 1.3, 1.5$, and we let $x_i=u_{i-100}$ for $i = 100$ to $n-100$. During the initial exploration, we found the transient states for each $c$ and discarded them for our analysis below.

\begin{figure}[h!]
  \centering
  
  \begin{subfigure}{0.45\textwidth}
    \includegraphics[width=\linewidth]{fig_contour05.png}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.45\textwidth}
    \includegraphics[width=\linewidth]{fig_contour12.png}
  \end{subfigure}

  \medskip

  \begin{subfigure}{0.45\textwidth}
    \includegraphics[width=\linewidth]{fig_contour13.png}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.45\textwidth}
    \includegraphics[width=\linewidth]{fig_contour15.png}
  \end{subfigure}

  \caption{Contour plots with time against $i$}
  \label{fig_contour}
\end{figure}

We observe from Figure \ref{fig_contour} that the solutions are oscillating initially for all $c$, and remain oscillating throughout time for $c=0.5$ and $c=1.2$, but the periodicity appears to break down gradually for $c=1.3$ at around $t=110$, and very quickly for $c=1.5$ at around $t=40$.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.7\linewidth]{fig_pattern13.png}
    \caption{Solution for $c=1.3$ against time}
    \label{fig_pattern13}
\end{figure}

Figure \ref{fig_pattern13} makes the sinusoidal oscillations very clear for $c=1.3$, and we observe the waves breaking down over time.

\begin{figure}[h!]
  \centering

  \begin{subfigure}{0.45\textwidth}
    \includegraphics[width=\linewidth]{fig_corr05.png}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.45\textwidth}
    \includegraphics[width=\linewidth]{fig_corr12.png}
  \end{subfigure}

  \medskip

  \begin{subfigure}{0.45\textwidth}
    \includegraphics[width=\linewidth]{fig_corr13.png}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.45\textwidth}
    \includegraphics[width=\linewidth]{fig_corr15.png}
  \end{subfigure}

  \caption{Correlation sum plots}
  \label{fig_corr}
\end{figure}

We calculate the correlation sum $C(\epsilon)$ (as seen in lecture slides) for each $c$. Figure \ref{fig_corr} shows the correlation sum plots and the corresponding least squares fit in log-log scale. The slope of this fit is the correlation dimension. The dimension is less than 2 for $c=0.5$ and $c=1.2$, and more than 2 for $c=1.3$ and $c=1.5$, suggesting the system is stable when $c\leq1.2$.

\FloatBarrier

We then apply PCA to $x$ and use the first component of the transformed data to spot patterns. 

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.7\linewidth]{fig_welch13}
    \caption{Power spectrum on first component of PCA-transformed data using Welch's method for $c=1.3$}
    \label{fig_welch13}
\end{figure}

Figure \ref{fig_welch13} shows the frequency spectrum at $c=1.3$ for the first component. We apply Welch's to all values of $c$ to estimate their dominant frequencies and time scales, these can be seen in Table \ref{tab_dom}. The time periods should approximately correspond to the period of the sinusoidal oscillations seen in Figure \ref{fig_contour}.

\begin{table}[h!]
    \centering
    \begin{tabular}{|c|c|c|}
        \hline
         & Dominant Time Period (1/f) \\
        \hline
        $c=0.5$ &  12.8\\
        $c=1.2$ &  5.33\\
        $c=1.3$ &  5.33\\
        $c=1.5$ &  4.92\\
        \hline
    \end{tabular}
    \caption{Dominant Time Period}
    \label{tab_dom}
\end{table}


\begin{figure}[h!]
  \centering

  \begin{subfigure}{0.49\textwidth}
    \includegraphics[width=\linewidth]{fig_pca05.png}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.49\textwidth}
    \includegraphics[width=\linewidth]{fig_pca12.png}
  \end{subfigure}

  \medskip

  \begin{subfigure}{0.49\textwidth}
    \includegraphics[width=\linewidth]{fig_pca13.png}
  \end{subfigure}
  \hfill 
  \begin{subfigure}{0.49\textwidth}
    \includegraphics[width=\linewidth]{fig_pca15.png}
  \end{subfigure}

  \caption{Phase plots and transition maps of the PCA-transformed data}
  \label{fig_PCA}
\end{figure}

Figure \ref{fig_PCA} shows the dynamics for each $c$. For the phase plots, we plot the scores of the second principal component $\tilde{x}_1$ against the scores of the first component $\tilde{x}_0$. For the transition maps, we take $\tau$ as $\frac{1}{5}$ of the dominant time scale derived from Table \ref{tab_dom} and use this to plot $\tilde{x}_0(t+\tau)$ against $\tilde{x}_0(t)$. These plots support our conclusion that the systems are stable for $c \leq 1.2$, and show that the systems exhibit chaotic behaviour for $c=1.3$ and $c=1.5$.

\FloatBarrier

\subsubsection*{2.}
%Place your brief discussion for question 2 here}
The 4 lines of code in function \textit{part3q2} apply principal component analysis to the solution matrix $\mathbf{A}$, then return a lower $x$-dimensional approximate reconstruction of $\mathbf{A}$ by applying Eckart-Young theorem, along with the error of the reconstruction. 
\begin{enumerate}
    \item The first line retrieves the principal components of $\mathbf{A}$ efficiently by finding $\mathbf{v_1}$, the eigenvectors of the covariance matrix $\mathbf{A}^\top\mathbf{A}$, using \textit{np.linalg.eigh}. Calculating the covariance matrix and its eigenvalues is computationally faster than the SVD algorithms we use in \textit{part1} for PCA, however, it is more memory-intensive and can be numerically unstable.
    \item The second line calculates $\mathbf{v_2}$, the transformed data matrix, by transforming $\mathbf{A}$ into the space spanned by the principal components using matrix multiplication.
    \item The third line uses the first $x$ columns of the transformed data matrix, along with the first $x$ principal components, to reconstruct the rank-$x$ approximation of $A$. This again uses matrix multiplication
    \item The last line calculates the square error of the reconstruction efficiently using \textit{NumPy} vectorised operations.
\end{enumerate}   

%---------------- End Part 3 -------------------
%Add additional figures as needed

\hrule
\hrule



%---------------- End document -------------------


\end{document}
